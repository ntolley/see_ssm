{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "# sys.path.append('../externals/mamba/')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import pickle\n",
    "import model_utils\n",
    "from torch import nn\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_fold = 0\n",
    "data_dict = model_utils.get_marker_decode_dataframes(noise_fold=noise_fold)\n",
    "wrist_df = data_dict['wrist_df']\n",
    "task_neural_df = data_dict['task_neural_df']\n",
    "notask_neural_df = data_dict['notask_neural_df']\n",
    "metadata = data_dict['metadata']\n",
    "cv_dict = data_dict['cv_dict']\n",
    "\n",
    "neuron_list = notask_neural_df['unit'].unique()\n",
    "\n",
    "notask_time_neural_mask = notask_neural_df['unit'] != 'time'\n",
    "notask_neural_df = notask_neural_df[notask_time_neural_mask]\n",
    "\n",
    "task_time_neural_mask = task_neural_df['unit'] != 'time'\n",
    "task_neural_df = task_neural_df[task_time_neural_mask]\n",
    "\n",
    "wrist_mask = wrist_df['name'] != 'time'\n",
    "wrist_df = wrist_df[wrist_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_offset = 10 # try 50-150 ms offset\n",
    "window_size = 70\n",
    "label_col = 'layout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM/GRU architecture for decoding\n",
    "#RNN architecture for decoding kinematics\n",
    "class model_mamba(nn.Module):\n",
    "    def __init__(self, input_size, output_size, d_model, d_state=16, d_conv=4, expand=2, dropout=0.2, device=device,\n",
    "                 cat_features=None):\n",
    "        super(model_mamba, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        self.cat_features = cat_features\n",
    "        self.input_size = input_size\n",
    "\n",
    "        if self.cat_features is not None:\n",
    "            self.num_cat_features = np.sum(self.cat_features).astype(int)\n",
    "            self.input_size = self.input_size - self.num_cat_features\n",
    "\n",
    "            \n",
    "        else:\n",
    "            self.fc = nn.Linear(in_features=d_model, out_features=output_size).to(device)\n",
    "\n",
    "        # self.fc = nn.Linear((input_), output_size)\n",
    "        self.mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        out = self.mamba(x)\n",
    "        out = self.fc(out)\n",
    "        return out, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn(pred_df, neural_df, neural_offset, cv_dict, metadata, task_info=True,\n",
    "            window_size=50, num_cat=0, label_col=None, flip_outputs=False, temperature=0.1, dropout=0.5):\n",
    "    exclude_processing = None\n",
    "    criterion = model_utils.mse\n",
    "    # if task_info:\n",
    "    #     criterion = partial(contrast_mse, temperature=temperature)\n",
    "    #     if num_cat > 0:\n",
    "    #         exclude_processing = np.zeros(len(neural_df['unit'].unique()))\n",
    "    #         exclude_processing[-num_cat:] = np.ones(num_cat)\n",
    "    #         exclude_processing = exclude_processing.astype(bool)\n",
    "\n",
    "    # else:\n",
    "    #     criterion = mse\n",
    "\n",
    "    data_arrays, generators = model_utils.make_generators(\n",
    "    pred_df, neural_df, neural_offset, cv_dict, metadata, exclude_neural=exclude_processing,\n",
    "    window_size=window_size, flip_outputs=flip_outputs, batch_size=1000, label_col=label_col)\n",
    "\n",
    "    # Unpack tuple into variables\n",
    "    training_set, validation_set, testing_set = data_arrays\n",
    "    training_generator, training_eval_generator, validation_generator, testing_generator = generators\n",
    "\n",
    "    X_train_data = training_set[:][0][:,-1,:].detach().cpu().numpy()\n",
    "    y_train_data = training_set[:][1][:,-1,:].detach().cpu().numpy()\n",
    "\n",
    "    X_test_data = testing_set[:][0][:,-1,:].detach().cpu().numpy()\n",
    "    y_test_data = testing_set[:][1][:,-1,:].detach().cpu().numpy()\n",
    "\n",
    "    #Define hyperparameters\n",
    "    lr = 1e-3\n",
    "    # weight_decay = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "    hidden_dim = 600\n",
    "    n_layers = 2\n",
    "    max_epochs = 1000\n",
    "    input_size = X_train_data.shape[1] \n",
    "    output_size = y_train_data.shape[1] \n",
    "\n",
    "    # model_rnn = model_lstm(input_size, output_size, hidden_dim, n_layers, dropout, device, cat_features=exclude_processing).to(device)\n",
    "    model_rnn = model_utils.model_lstm(input_size, output_size, hidden_dim, n_layers, dropout, device, cat_features=exclude_processing).to(device)\n",
    "\n",
    "    # model_rnn = model_mamba(input_size, output_size, d_model=input_size, d_state=128, d_conv=4, expand=2)\n",
    "\n",
    "\n",
    "    # Define Loss, Optimizerints h\n",
    "    optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    #Train model\n",
    "    loss_dict = model_utils.train_validate_model(model_rnn, optimizer, criterion, max_epochs, training_generator, validation_generator, device, 10, 5)\n",
    "\n",
    "    #Evaluate trained model\n",
    "    rnn_train_pred = model_utils.evaluate_model(model_rnn, training_eval_generator, device)\n",
    "    rnn_test_pred = model_utils.evaluate_model(model_rnn, testing_generator, device)\n",
    "\n",
    "    rnn_train_corr = model_utils.matrix_corr(rnn_train_pred, y_train_data)\n",
    "    rnn_test_corr = model_utils.matrix_corr(rnn_test_pred, y_test_data)\n",
    "\n",
    "    res_dict = {'loss_dict': loss_dict,\n",
    "                'train_pred': rnn_train_pred, 'test_pred': rnn_test_pred,\n",
    "                'train_corr': rnn_train_corr, 'test_corr': rnn_test_corr}\n",
    "\n",
    "    return model_rnn, res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f5074a09e10>\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/ntolley/.conda/envs/see/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".."
     ]
    }
   ],
   "source": [
    "# func_dict = {'wiener': contrastive_functions.run_wiener, 'rnn': contrastive_functions.run_rnn}\n",
    "func_dict = {'rnn': run_rnn}\n",
    "\n",
    "fpath = '../data/neuron_num_results/'\n",
    "\n",
    "num_repeats = 1\n",
    "\n",
    "num_neuron_results_dict = {'noise_fold': noise_fold}\n",
    "\n",
    "# Filter neural_df with task info to random subset of neurons\n",
    "layout_mask = task_neural_df['unit'].str.contains(pat='layout')\n",
    "task_neural_df_filtered = task_neural_df[np.logical_or.reduce([layout_mask])].reset_index(drop=True)\n",
    "\n",
    "# df_dict = {'task': {'df': task_neural_df_filtered, 'task_info': True, 'num_cat': 4, 'flip_outputs': True},\n",
    "#             'notask': {'df': notask_neural_df_filtered, 'task_info': False, 'num_cat': 0, 'flip_outputs': True}}\n",
    "\n",
    "df_dict = {'notask': {'df': notask_neural_df, 'task_info': False, 'num_cat': 0, 'flip_outputs': True}}\n",
    "\n",
    "decode_results = dict()\n",
    "for func_name, func in func_dict.items():\n",
    "    decode_results[func_name] = dict()\n",
    "    for df_type, pred_df in df_dict.items():\n",
    "        # print(f'{func_name}_{df_type} num_neurons: {num_neurons}; repeat {repeat_idx}')\n",
    "\n",
    "        model, res_dict = func(wrist_df, pred_df['df'], neural_offset, cv_dict, metadata, task_info=pred_df['task_info'],\n",
    "                                window_size=window_size, num_cat=pred_df['num_cat'], label_col=label_col, flip_outputs=pred_df['flip_outputs'])\n",
    "\n",
    "        decode_results[func_name][df_type] = res_dict\n",
    "\n",
    "        # # Save results on every loop in case early stop\n",
    "        # num_neuron_results_dict[f'repeat_{repeat_idx}'][f'num_neuron_{num_neurons}'] = decode_results\n",
    "        # # #Save metadata\n",
    "        # output = open(f'{fpath}num_neuron_results.pkl', 'wb')\n",
    "        # pickle.dump(num_neuron_results_dict, output)\n",
    "        # output.close()\n",
    "\n",
    "        # if func_name == 'rnn':\n",
    "        #     torch.save(model.state_dict(), f'{fpath}models/{df_type}_neurons{num_neurons}_repeat{repeat_idx}.pt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78619208, 0.85263139, 0.78664979, 0.77634815, 0.85054791,\n",
       "       0.80889051, 0.75773891, 0.84999737, 0.81096381, 0.7477906 ,\n",
       "       0.84765538, 0.81446937, 0.32611803, 0.60442949, 0.5691553 ,\n",
       "       0.70905636, 0.79873693, 0.65244833, 0.82744105, 0.87781194,\n",
       "       0.78477901, 0.84170869, 0.88496196, 0.80491002, 0.79456252,\n",
       "       0.8728516 , 0.83119905, 0.83054333, 0.87969224, 0.84003424,\n",
       "       0.8473001 , 0.88312128, 0.84975987, 0.85022376, 0.88566084,\n",
       "       0.85071061, 0.79987327, 0.86219309, 0.79723843, 0.81339499,\n",
       "       0.86437358, 0.682533  , 0.8272171 , 0.8683661 , 0.52161455,\n",
       "       0.82627647, 0.82650184, 0.35144037, 0.8014648 , 0.8519737 ,\n",
       "       0.78406712, 0.82789316, 0.85126832, 0.80678569, 0.83949846,\n",
       "       0.85967821, 0.81182206, 0.84359117, 0.86252679, 0.81172522,\n",
       "       0.7609846 , 0.82993026, 0.71672999, 0.82295128, 0.880294  ,\n",
       "       0.69096293])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict['test_corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "see",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
