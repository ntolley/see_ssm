{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import pickle\n",
    "import model_utils\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_fold = 0\n",
    "data_dict = model_utils.get_marker_decode_dataframes(noise_fold=noise_fold)\n",
    "wrist_df = data_dict['wrist_df']\n",
    "task_neural_df = data_dict['task_neural_df']\n",
    "notask_neural_df = data_dict['notask_neural_df']\n",
    "metadata = data_dict['metadata']\n",
    "cv_dict = data_dict['cv_dict']\n",
    "\n",
    "neuron_list = notask_neural_df['unit'].unique()\n",
    "\n",
    "notask_time_neural_mask = notask_neural_df['unit'] != 'time'\n",
    "notask_neural_df = notask_neural_df[notask_time_neural_mask]\n",
    "\n",
    "task_time_neural_mask = task_neural_df['unit'] != 'time'\n",
    "task_neural_df = task_neural_df[task_time_neural_mask]\n",
    "\n",
    "wrist_mask = wrist_df['name'] != 'time'\n",
    "wrist_df = wrist_df[wrist_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_offset = 10 # try 50-150 ms offset\n",
    "window_size = 70\n",
    "label_col = 'layout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn(pred_df, neural_df, neural_offset, cv_dict, metadata, task_info=True,\n",
    "            window_size=10, num_cat=0, label_col=None, flip_outputs=False, temperature=0.1, dropout=0.5):\n",
    "    exclude_processing = None\n",
    "    criterion = model_utils.mse\n",
    "    # if task_info:\n",
    "    #     criterion = partial(contrast_mse, temperature=temperature)\n",
    "    #     if num_cat > 0:\n",
    "    #         exclude_processing = np.zeros(len(neural_df['unit'].unique()))\n",
    "    #         exclude_processing[-num_cat:] = np.ones(num_cat)\n",
    "    #         exclude_processing = exclude_processing.astype(bool)\n",
    "\n",
    "    # else:\n",
    "    #     criterion = mse\n",
    "\n",
    "    data_arrays, generators = model_utils.make_generators(\n",
    "    pred_df, neural_df, neural_offset, cv_dict, metadata, exclude_neural=exclude_processing,\n",
    "    window_size=window_size, flip_outputs=flip_outputs, batch_size=1000, label_col=label_col)\n",
    "\n",
    "    # Unpack tuple into variables\n",
    "    training_set, validation_set, testing_set = data_arrays\n",
    "    training_generator, training_eval_generator, validation_generator, testing_generator = generators\n",
    "\n",
    "    X_train_data = training_set[:][0][:,-1,:].detach().cpu().numpy()\n",
    "    y_train_data = training_set[:][1][:,-1,:].detach().cpu().numpy()\n",
    "\n",
    "    X_test_data = testing_set[:][0][:,-1,:].detach().cpu().numpy()\n",
    "    y_test_data = testing_set[:][1][:,-1,:].detach().cpu().numpy()\n",
    "\n",
    "    #Define hyperparameters\n",
    "    lr = 1e-4\n",
    "    # weight_decay = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "    hidden_dim = 300\n",
    "    n_layers = 2\n",
    "    max_epochs = 1000\n",
    "    input_size = X_train_data.shape[1] \n",
    "    output_size = y_train_data.shape[1] \n",
    "\n",
    "    # model_rnn = model_lstm(input_size, output_size, hidden_dim, n_layers, dropout, device, cat_features=exclude_processing).to(device)\n",
    "    model_rnn = model_utils.model_lstm(input_size, output_size, hidden_dim, n_layers, dropout, device, cat_features=exclude_processing).to(device)\n",
    "\n",
    "    # Define Loss, Optimizerints h\n",
    "    optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    #Train model\n",
    "    loss_dict = model_utils.train_validate_model(model_rnn, optimizer, criterion, max_epochs, training_generator, validation_generator, device, 10, 5)\n",
    "\n",
    "    #Evaluate trained model\n",
    "    rnn_train_pred = model_utils.evaluate_model(model_rnn, training_eval_generator, device)\n",
    "    rnn_test_pred = model_utils.evaluate_model(model_rnn, testing_generator, device)\n",
    "\n",
    "    rnn_train_corr = mocap_functions.matrix_corr(rnn_train_pred, y_train_data)\n",
    "    rnn_test_corr = mocap_functions.matrix_corr(rnn_test_pred, y_test_data)\n",
    "\n",
    "    res_dict = {'loss_dict': loss_dict,\n",
    "                'train_pred': rnn_train_pred, 'test_pred': rnn_test_pred,\n",
    "                'train_corr': rnn_train_corr, 'test_corr': rnn_test_corr}\n",
    "\n",
    "    return model_rnn, res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ntolley/.conda/envs/see/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 48 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/users/ntolley/.conda/envs/see/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 48 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/users/ntolley/.conda/envs/see/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 48 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******.*.*\n",
      "Epoch: 10/1000 ... Train Loss: 0.2858  ... Validation Loss: 0.3641\n",
      ".*.*****..\n",
      "Epoch: 20/1000 ... Train Loss: 0.1632  ... Validation Loss: 0.2779\n",
      ".***...*..\n",
      "Epoch: 30/1000 ... Train Loss: 0.1181  ... Validation Loss: 0.2475\n",
      "..*...... Early Stop; Min Epoch: 33\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m decode_results[func_name] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m df_type, pred_df \u001b[39min\u001b[39;00m df_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# print(f'{func_name}_{df_type} num_neurons: {num_neurons}; repeat {repeat_idx}')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     model, res_dict \u001b[39m=\u001b[39m func(wrist_df, pred_df[\u001b[39m'\u001b[39;49m\u001b[39mdf\u001b[39;49m\u001b[39m'\u001b[39;49m], neural_offset, cv_dict, metadata, task_info\u001b[39m=\u001b[39;49mpred_df[\u001b[39m'\u001b[39;49m\u001b[39mtask_info\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m                             window_size\u001b[39m=\u001b[39;49mwindow_size, num_cat\u001b[39m=\u001b[39;49mpred_df[\u001b[39m'\u001b[39;49m\u001b[39mnum_cat\u001b[39;49m\u001b[39m'\u001b[39;49m], label_col\u001b[39m=\u001b[39;49mlabel_col, flip_outputs\u001b[39m=\u001b[39;49mpred_df[\u001b[39m'\u001b[39;49m\u001b[39mflip_outputs\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     decode_results[func_name][df_type] \u001b[39m=\u001b[39m res_dict\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# # Save results on every loop in case early stop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# num_neuron_results_dict[f'repeat_{repeat_idx}'][f'num_neuron_{num_neurons}'] = decode_results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m# # #Save metadata\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m# if func_name == 'rnn':\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m#     torch.save(model.state_dict(), f'{fpath}models/{df_type}_neurons{num_neurons}_repeat{repeat_idx}.pt')\u001b[39;00m\n",
      "\u001b[1;32m/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m loss_dict \u001b[39m=\u001b[39m model_utils\u001b[39m.\u001b[39mtrain_validate_model(model_rnn, optimizer, criterion, max_epochs, training_generator, validation_generator, device, \u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m#Evaluate trained model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m rnn_train_pred \u001b[39m=\u001b[39m model_utils\u001b[39m.\u001b[39;49mevaluate_model(model_rnn, training_eval_generator, device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m rnn_test_pred \u001b[39m=\u001b[39m model_utils\u001b[39m.\u001b[39mevaluate_model(model_rnn, testing_generator, device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/Donoghue_Lab/see_ssm/notebooks/train_decoder.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m rnn_train_corr \u001b[39m=\u001b[39m mocap_functions\u001b[39m.\u001b[39mmatrix_corr(rnn_train_pred, y_train_data)\n",
      "File \u001b[0;32m/oscar/home/ntolley/Donoghue_Lab/see_ssm/notebooks/../code/model_utils.py:454\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, generator, device)\u001b[0m\n\u001b[1;32m    452\u001b[0m y_pred_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(generator\u001b[39m.\u001b[39mdataset),  generator\u001b[39m.\u001b[39mdataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m    453\u001b[0m batch_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 454\u001b[0m \u001b[39mfor\u001b[39;00m batch_x, batch_y \u001b[39min\u001b[39;00m generator:\n\u001b[1;32m    455\u001b[0m     batch_x \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    456\u001b[0m     batch_y \u001b[39m=\u001b[39m batch_y\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# func_dict = {'wiener': contrastive_functions.run_wiener, 'rnn': contrastive_functions.run_rnn}\n",
    "func_dict = {'rnn': run_rnn}\n",
    "\n",
    "fpath = '../data/neuron_num_results/'\n",
    "\n",
    "num_repeats = 1\n",
    "\n",
    "num_neuron_results_dict = {'noise_fold': noise_fold}\n",
    "\n",
    "# Filter neural_df with task info to random subset of neurons\n",
    "layout_mask = task_neural_df['unit'].str.contains(pat='layout')\n",
    "task_neural_df_filtered = task_neural_df[np.logical_or.reduce([layout_mask])].reset_index(drop=True)\n",
    "\n",
    "# df_dict = {'task': {'df': task_neural_df_filtered, 'task_info': True, 'num_cat': 4, 'flip_outputs': True},\n",
    "#             'notask': {'df': notask_neural_df_filtered, 'task_info': False, 'num_cat': 0, 'flip_outputs': True}}\n",
    "\n",
    "df_dict = {'notask': {'df': notask_neural_df, 'task_info': False, 'num_cat': 0, 'flip_outputs': True}}\n",
    "\n",
    "decode_results = dict()\n",
    "for func_name, func in func_dict.items():\n",
    "    decode_results[func_name] = dict()\n",
    "    for df_type, pred_df in df_dict.items():\n",
    "        # print(f'{func_name}_{df_type} num_neurons: {num_neurons}; repeat {repeat_idx}')\n",
    "\n",
    "        model, res_dict = func(wrist_df, pred_df['df'], neural_offset, cv_dict, metadata, task_info=pred_df['task_info'],\n",
    "                                window_size=window_size, num_cat=pred_df['num_cat'], label_col=label_col, flip_outputs=pred_df['flip_outputs'])\n",
    "\n",
    "        decode_results[func_name][df_type] = res_dict\n",
    "\n",
    "        # # Save results on every loop in case early stop\n",
    "        # num_neuron_results_dict[f'repeat_{repeat_idx}'][f'num_neuron_{num_neurons}'] = decode_results\n",
    "        # # #Save metadata\n",
    "        # output = open(f'{fpath}num_neuron_results.pkl', 'wb')\n",
    "        # pickle.dump(num_neuron_results_dict, output)\n",
    "        # output.close()\n",
    "\n",
    "        # if func_name == 'rnn':\n",
    "        #     torch.save(model.state_dict(), f'{fpath}models/{df_type}_neurons{num_neurons}_repeat{repeat_idx}.pt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "see",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
